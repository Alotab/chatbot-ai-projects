{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8620d1b4",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain\n",
    "pip install pinecone-client\n",
    "pip install openai\n",
    "pip install tiktoken\n",
    "pip install nest_asyncio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc4b4021",
   "metadata": {},
   "source": [
    "# Set up OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b91b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Your API KEY here\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "806a7d54",
   "metadata": {},
   "source": [
    "# Set up Pinecone API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f795a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "#initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=\"your langchain api key here\", \n",
    "    environment=\"your environment name\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef5eae32",
   "metadata": {},
   "source": [
    "# Index\n",
    "Load data from 'https://ind.nl'\n",
    "Extends from the WebBaselLoader, this will load a sitemap from a given URL, and then # scrape and load all the pages in the sitemap, returing each page as a document.\n",
    "\n",
    "This scraping is done concurrently, using WebBaselLoader. There are reasonable limits to concurrent requests, defaulting to 2 per second.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dd43b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|#########################| 509/509 [03:49<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from langchain.document_loaders.sitemap import SitemapLoader\n",
    "\n",
    "loader = SitemapLoader(\n",
    "    \"https://ind.nl/sitemap.xml\",\n",
    "    filter_urls=['https://ind.nl/en']\n",
    "\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bac3644",
   "metadata": {},
   "source": [
    "## Text split\n",
    "split the text from the docs into smaller chunks\n",
    "There are many ways to split the text. We are using the text splitter that is recommended for generic texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b248d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1200,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "docs_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b58269b6",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b27f3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "124e6282",
   "metadata": {},
   "source": [
    "## Creating a vectorstore\n",
    "A vector stores Documents and associated embeddings, and provides fast ways to look up relevant\n",
    "Documents by embeddings. There are many ways to create a vectorstore. We are going to use Pinecone. First create a new index in Pinecone and type the name in \"index_name\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a76a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "# get this name from the index name you just created on Pinecone\n",
    "index_name = \"ind\"\n",
    "\n",
    "## create a new index\n",
    "docsearch = Pinecone.from_documents(docs_chunks, embeddings, index_name=index_name)\n",
    "\n",
    "#if you already have an index, you can load it like this\n",
    "#docsearch = Pinecone.from_existing_index(index_name, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5104ce06",
   "metadata": {},
   "source": [
    "## vector is ready. \n",
    "Let's try to query dosearch with similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "800ffba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "query = \"How to get a visa for my partner if I have a highly skilled visa.\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs[0])\n",
    "#print(len(docs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2f9cf6c",
   "metadata": {},
   "source": [
    "## Making a question answering chain \n",
    "The question chain will enable us to generate the answer based on the relevant context chucnks\n",
    "Additionally, we can return the source of the document used to answer the question by specifying an optional parameter when constructing the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4d226fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "llm=OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_response = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", \n",
    "                                               retriever=docsearch.as_retriever(),\n",
    "                                              return_source_documents=True\n",
    "                                              )\n",
    "\n",
    "query = \"How to get a visa for my parents if I have a highly skilled visa\"\n",
    "result = qa_with_response({'query': query})\n",
    "\n",
    "# Output text result that was foung for the query\n",
    "result['result']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3729323d",
   "metadata": {},
   "source": [
    "## Output source documents that were found for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['source_documents']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
